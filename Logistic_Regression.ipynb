{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4PB0MjOjzmjzK5Br7WgLP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajnish379/Machine_Learning/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEuFkl8MkHPp",
        "outputId": "ffd8a66c-5b15-407d-e438-3edfb2aa1d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class setosa:\n",
            "Weights: [-0.42321659  0.96692251 -2.51673255 -1.08077345]\n",
            "Odds Ratios: [0.65493676 2.62983869 0.08072293 0.33933297]\n",
            "\n",
            "Class versicolor:\n",
            "Weights: [ 0.53433525 -0.32159022 -0.20662058 -0.94364829]\n",
            "Odds Ratios: [1.70631358 0.72499522 0.81332818 0.38920531]\n",
            "\n",
            "Class virginica:\n",
            "Weights: [-0.11111865 -0.64533229  2.72335313  2.02442173]\n",
            "Odds Ratios: [ 0.89483257  0.52448823 15.23130932  7.57173119]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=200)\n",
        "logreg.fit(X, y)\n",
        "\n",
        "\n",
        "# Logistic regression optimization iteratively adjusts parameters to maximize log-likelihood,\n",
        "# targeting where the first derivative (gradient) is zero. Different solvers, like lbfgs, newton-cg,\n",
        "# liblinear, sag, and saga, apply distinct methods to estimate and leverage derivatives. Dataset specifics,\n",
        "# feature characteristics, and numerical precision influence convergence. The 'max_iter' parameter sets a\n",
        "# ceiling on the number of iterations, terminating the algorithm when it has adequately approximated the\n",
        "# log-likelihood's maximum, thus ensuring a balance between search precision and computational efficiency\n",
        "# across varying solvers. For instance, the Newton-Raphson method, by seeking parameter values that zero\n",
        "# the first derivative, resembles a binary search in refining the interval of search via the second derivative\n",
        "# to inform step size adjustments.\n",
        "\n",
        "# After optimization, we can interpret the coefficients of the learned logistic regression model:\n",
        "# The \"weights\" array is of shape 3-by-4 because there are 3 classes and 4 features in the dataset.\n",
        "# exp(weights[c][i]) gives the odds ratio for the i-th feature in discriminating the c-th class against all other classes.\n",
        "# This implies that for a one-unit increase in the i-th feature, the odds of the sample being in class c (as opposed to any other class) are multiplied by exp(weights[c][i]).\n",
        "# An exp(weights[c][i]) value greater than 1 indicates that the odds increase with the feature's value; if it is less than 1, the odds decrease.\n",
        "weights = logreg.coef_\n",
        "\n",
        "odds_ratios = np.exp(weights)\n",
        "# The raw weights in logistic regression quantify the additive change in log-odds for a one-unit increase in the corresponding feature value. However, to capture the multiplicative effect on the odds themselves, we exponentiate the weights.\n",
        "# This transformation shifts the interpretation from an additive change in log-odds to a multiplicative change in the actual odds ratio for each feature as previously detailed above.\n",
        "\n",
        "# Print the weights and odds ratios for each class\n",
        "for i, class_label in enumerate(logreg.classes_):\n",
        "    print(f\"Class {iris.target_names[class_label]}:\")\n",
        "    print(f\"Weights: {weights[i]}\")\n",
        "    print(f\"Odds Ratios: {odds_ratios[i]}\")\n",
        "    print()"
      ]
    }
  ]
}